{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5fae0aa",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9fd51",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27195278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import boto3\n",
    "\n",
    "import numpy as np\n",
    "import pyspark\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "from utils import process_review, build_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d500cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# set up a giant single executor with many threads and specify memory cap\n",
    "spark = pyspark.sql.SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.executor.instances\", 10) \\\n",
    "        .config(\"spark.driver.memory\", \"32g\") \\\n",
    "        .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.analyzer.failAmbiguousSelfJoin\", \"false\")\n",
    "\n",
    "BUCKET_NAME = 'amazon-reviews-pds'\n",
    "LOCAL_DIR = os.path.expanduser('~') + '/SageMaker/data/'\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(BUCKET_NAME)\n",
    "\n",
    "first = True\n",
    "\n",
    "for object_summary in my_bucket.objects.filter(Prefix=\"parquet/product_category=Electronics/\"):\n",
    "    OBJECT_NAME = object_summary.key\n",
    "    LOCAL_FILE = LOCAL_DIR + OBJECT_NAME.split(\"/\")[-1]\n",
    "    s3_client.download_file(BUCKET_NAME, OBJECT_NAME, LOCAL_FILE)\n",
    "    data = spark.read.parquet(LOCAL_FILE)\n",
    "    print(data.count())\n",
    "    if first:\n",
    "        first = False\n",
    "    else:\n",
    "        data = prev_data.union(data)\n",
    "    prev_data = data\n",
    "\n",
    "print(data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c44de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd41691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "data=data.select('review_body', 'star_rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76aa913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "data = data.filter(data['review_body'].isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702cf931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# Positive reviews have a rating greater than 3, and\n",
    "# negative reviews have a rating less than 3\n",
    "positive_reviews = data.where(data['star_rating'] > 3)\n",
    "# assign sentiment value of 1\n",
    "positive_reviews=positive_reviews.withColumn(\"sentiment\", lit(1))\n",
    "NUM_POSITIVE_REVIEWS = positive_reviews.count()\n",
    "print(\"Number of positive reviews: \", NUM_POSITIVE_REVIEWS)\n",
    "negative_reviews = data.where(data['star_rating'] < 3)\n",
    "# assign sentiment value of 0\n",
    "negative_reviews=negative_reviews.withColumn(\"sentiment\", lit(0))\n",
    "NUM_NEGATIVE_REVIEWS = negative_reviews.count()\n",
    "print(\"Number of negative reviews: \", NUM_NEGATIVE_REVIEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a719ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# free up data\n",
    "data=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3ec6fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "reviews = positive_reviews.union(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd780c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "reviews = reviews.select(\"*\").withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74fb9b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "positive_reviews=0\n",
    "negative_reviews=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce7aa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tError sending http request and maximum retry encountered..\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "# Declare a figure with a custom size\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# labels for the two classes\n",
    "labels = 'Positives', 'Negative'\n",
    "\n",
    "# Sizes for each slide\n",
    "sizes = [NUM_POSITIVE_REVIEWS, NUM_NEGATIVE_REVIEWS] \n",
    "\n",
    "# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.axis('equal')  \n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ee434",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import rand\n",
    "[t[0] for t in positive_reviews.select('review_body').orderBy(rand()).limit(3).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd59dac",
   "metadata": {},
   "source": [
    "[t[0] for t in negative_reviews.select('review_body').orderBy(rand()).limit(3).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0dbb8e",
   "metadata": {},
   "source": [
    "## Preprocess raw text for sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc340b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer_5e520cd27190"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(outputCol=\"words\")\n",
    "tokenizer.setInputCol(\"review_body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65153358",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = tokenizer.transform(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2581dac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+---+--------------------+\n",
      "|         review_body|star_rating|sentiment| id|               words|\n",
      "+--------------------+-----------+---------+---+--------------------+\n",
      "|These headphones ...|          4|        1|  0|[these, headphone...|\n",
      "|Arrived as expect...|          5|        1|  1|[arrived, as, exp...|\n",
      "|the reason I chos...|          4|        1|  2|[the, reason, i, ...|\n",
      "+--------------------+-----------+---------+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a244b90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+---+--------------------+--------------------+\n",
      "|         review_body|star_rating|sentiment| id|               words|              tokens|\n",
      "+--------------------+-----------+---------+---+--------------------+--------------------+\n",
      "|These headphones ...|          4|        1|  0|[these, headphone...|[headphones, nice...|\n",
      "|Arrived as expect...|          5|        1|  1|[arrived, as, exp...|[arrived, expecte...|\n",
      "|the reason I chos...|          4|        1|  2|[the, reason, i, ...|[reason, chose, p...|\n",
      "+--------------------+-----------+---------+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"tokens\")\n",
    "reviews = remover.transform(reviews)\n",
    "reviews.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "661eb8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "reviews = reviews.withColumn(\"stemmed_tokens\", stemmer_udf(\"tokens\")).select('id', 'stemmed_tokens','sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bcb467b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n",
      "| id|      stemmed_tokens|sentiment|\n",
      "+---+--------------------+---------+\n",
      "|  0|[headphon, nice.,...|        1|\n",
      "|  1|[arriv, expect, g...|        1|\n",
      "|  2|[reason, chose, p...|        1|\n",
      "+---+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2037299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
